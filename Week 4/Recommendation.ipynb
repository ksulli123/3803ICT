{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[4. 3. 2. 3.]\n",
      "[1. 2. 3. 1.]\n",
      "[[ 4.  3.  2.  3.]\n",
      " [ 1.  2.  3.  1.]\n",
      " [nan  2.  1. nan]\n",
      " [ 4.  3. nan nan]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: load dataset into variable M\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "NaN = np.nan\n",
    "\n",
    "M = np.array([\n",
    "    [4,    3, 2,    3],\n",
    "    [1,    2, 3,    1],\n",
    "    [NaN, 2, 1,    NaN],\n",
    "    [4,    3, NaN, NaN]\n",
    "], dtype=np.float)\n",
    "\n",
    "print(M.shape)\n",
    "print(M[0,:])\n",
    "print(M[1,:])\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(v1,v2, metric='cosine'):\n",
    "    #metric: cosine or correlation\n",
    "    if metric == 'correlation':\n",
    "        v1 = v1 - np.nanmean(v1)\n",
    "        v2 = v2 - np.nanmean(v2)\n",
    "    \"compute similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        if np.isnan(x) or np.isnan(y): continue\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def sim_matrix(M, dimension='user', metric='cosine'):\n",
    "    N = M.shape[0] if dimension == 'user' else M.shape[1]\n",
    "    sim = np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                sim[i,j] = 0 #Cancel out the effect of self-similarity in the sums later\n",
    "                continue\n",
    "            if dimension == 'user':\n",
    "                v1, v2 = M[i,:], M[j,:]\n",
    "            else:\n",
    "                v1, v2 = M[:,i], M[:,j]\n",
    "            sim[i][j] = cosine_similarity(v1,v2,metric)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79582243, 0.99227788, 1.        ],\n",
       "       [0.79582243, 0.        , 0.86824314, 0.89442719],\n",
       "       [0.99227788, 0.86824314, 0.        , 1.        ],\n",
       "       [1.        , 0.89442719, 1.        , 0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.9649505 , 0.73994007, 0.99705449],\n",
       "       [0.9649505 , 0.        , 0.90748521, 0.96476382],\n",
       "       [0.73994007, 0.90748521, 0.        , 0.78935222],\n",
       "       [0.99705449, 0.96476382, 0.78935222, 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(M[0,:], M[2,:], 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.85280287,  0.70710678,  0.70710678],\n",
       "       [-0.85280287,  0.        , -0.5547002 , -0.89442719],\n",
       "       [ 0.70710678, -0.5547002 ,  0.        , -1.        ],\n",
       "       [ 0.70710678, -0.89442719, -1.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix(M, 'user', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.79582243, 0.99227788, 1.        ],\n",
       "       [0.79582243, 0.        , 0.86824314, 0.89442719],\n",
       "       [0.99227788, 0.86824314, 0.        , 1.        ],\n",
       "       [1.        , 0.89442719, 1.        , 0.        ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_users = sim_matrix(M, 'user', 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Compute the missing rating in this table using user-based collaborative filtering (CF). (Use cosine similarity, then use Pearson similarity). Assume taking all neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=1)\n",
    "    sim_users = sim_matrix(M, 'user', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                pred[i,j] = sum([sim_users[user, j] for user in range(n_users)])\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF (Cosine): \n",
      "        0    1         2         3\n",
      "0  4.0000  3.0  2.000000  3.000000\n",
      "1  1.0000  2.0  3.000000  1.000000\n",
      "2  2.7881  2.0  1.000000  2.894427\n",
      "3  4.0000  3.0  2.860521  2.894427\n",
      "User-based CF (Pearson): \n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  0.561411  2.0  1.000000 -1.18732\n",
      "3  4.000000  3.0 -0.847593 -1.18732\n"
     ]
    }
   ],
   "source": [
    "print(\"User-based CF (Cosine): \\n\" + str(pd.DataFrame(user_cf(M, 'cosine'))))\n",
    "print(\"User-based CF (Pearson): \\n\" + str(pd.DataFrame(user_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Similarly, computing the missing rating using item-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_cf(M, metric='cosine'):\n",
    "    pred = np.copy(M)\n",
    "    n_users, n_items = M.shape\n",
    "    avg_ratings = np.nanmean(M, axis=0)\n",
    "    sim_items = sim_matrix(M, 'item', metric)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            if np.isnan(M[i,j]):\n",
    "                pred[i,j] = sum([sim_items[i, item] for item in range(n_items)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF (Cosine): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  2.436778  2.0  1.000000  2.436778\n",
      "3  4.000000  3.0  2.751171  2.751171\n",
      "Item-based CF (Pearson): \n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2 -1.601534  2.0  1.000000 -1.601534\n",
      "3  4.000000  3.0  1.241577  1.241577\n"
     ]
    }
   ],
   "source": [
    "print(\"Item-based CF (Cosine): \\n\" + str(pd.DataFrame(item_cf(M, 'cosine'))))\n",
    "print(\"Item-based CF (Pearson): \\n\" + str(pd.DataFrame(item_cf(M, 'correlation'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Recommendation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  4  3  2  3\n",
       "1  1  2  3  1\n",
       "2  1  2  1  2\n",
       "3  4  3  2  4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_result = np.asarray([[4,3,2,3], \n",
    "                [1,2,3,1],\n",
    "                [1,2,1,2],\n",
    "                [4,3,2,4]])\n",
    "pd.DataFrame(M_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRS(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #method: user_cf and item_cf, metric: cosine and correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    MSE = mean_squared_error(prediction, groundtruth)\n",
    "    RMSE = round(math.sqrt(MSE),3)\n",
    "    print(\"RMSE using {0} approach ({2}) is: {1}\".format(method, RMSE, metric))\n",
    "    print(pd.DataFrame(prediction))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using user_cf approach (cosine) is: 0.61\n",
      "        0    1         2         3\n",
      "0  4.0000  3.0  2.000000  3.000000\n",
      "1  1.0000  2.0  3.000000  1.000000\n",
      "2  2.7881  2.0  1.000000  2.894427\n",
      "3  4.0000  3.0  2.860521  2.894427\n",
      "RMSE using user_cf approach (correlation) is: 1.684\n",
      "          0    1         2        3\n",
      "0  4.000000  3.0  2.000000  3.00000\n",
      "1  1.000000  2.0  3.000000  1.00000\n",
      "2  0.561411  2.0  1.000000 -1.18732\n",
      "3  4.000000  3.0 -0.847593 -1.18732\n",
      "RMSE using item_cf approach (cosine) is: 0.523\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2  2.436778  2.0  1.000000  2.436778\n",
      "3  4.000000  3.0  2.751171  2.751171\n",
      "RMSE using item_cf approach (correlation) is: 1.321\n",
      "          0    1         2         3\n",
      "0  4.000000  3.0  2.000000  3.000000\n",
      "1  1.000000  2.0  3.000000  1.000000\n",
      "2 -1.601534  2.0  1.000000 -1.601534\n",
      "3  4.000000  3.0  1.241577  1.241577\n"
     ]
    }
   ],
   "source": [
    "#TODO: evaluate the predictive accuracy\n",
    "evaluateRS(M, M_result, method=\"user_cf\", metric=\"cosine\")\n",
    "evaluateRS(M, M_result, method=\"user_cf\", metric=\"correlation\")\n",
    "evaluateRS(M, M_result, method=\"item_cf\", metric=\"cosine\")\n",
    "evaluateRS(M, M_result, method=\"item_cf\", metric=\"correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking acc using item_cf approach (correlation) is: 0.6559016994374948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6559016994374948"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def evaluate_rank(ratings, groundtruth, method='user_cf', metric='cosine'):\n",
    "    #metric: cosine vs correlation\n",
    "    if method == 'user_cf':\n",
    "        prediction = user_cf(ratings, metric)\n",
    "    else:\n",
    "        prediction = item_cf(ratings, metric)\n",
    "    \n",
    "    avg_tau = 0\n",
    "    n_users, n_items = M.shape\n",
    "    for i in range(n_users):\n",
    "        tau, p_value = stats.kendalltau(M_result[i,:], prediction[i,:])\n",
    "        avg_tau += tau\n",
    "    avg_tau = avg_tau / n_users\n",
    "    clear_output(wait=True)\n",
    "    print(\"Rank accuracy of \" + method + \" with \" + metric + \" metric: \" + str(avg_tau))\n",
    "    return avg_tau\n",
    "\n",
    "\n",
    "#TODO: calculate the ranking accuracy\n",
    "evaluate_rank(M, M_result, method=\"user_cf\", metric=\"cosine\")\n",
    "evaluate_rank(M, M_result, method=\"user_cf\", metric=\"correlation\")\n",
    "evaluate_rank(M, M_result, method=\"item_cf\", metric=\"cosine\")\n",
    "evaluate_rank(M, M_result, method=\"item_cf\", metric=\"correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
